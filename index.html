<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Required meta tags always come first -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css">
    <link rel="stylesheet" href="./css/styles.css">

    <link rel="shortcut icon" href="https://www.cl.cam.ac.uk/favicon.ico">

    <title>Chi Ian Tang - AI/ML Researcher in Mobile Systems</title>

</head>

<body data-spy="scroll" data-target=".navbar" data-offset="200">

    <nav class="navbar navbar-dark navbar-expand-sm fixed-top">
        <div class="container">
            <div class="navbar-header">
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#Navbar">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <a class="navbar-brand mx-auto align-middle" href="#top">Chi Ian Tang</a>
            </div>
            <div class="collapse navbar-collapse" id="Navbar">
                <ul class="nav navbar-nav mr-auto">
                    <li class="nav-item"><a class="nav-link" href="#top"> About</a></li>
                    <li class="nav-item"><a class="nav-link" href="#news">News</a></li>
                    <li class="nav-item"><a class="nav-link" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link" href="#service">Service</a></li>
                    <li class="nav-item"><a class="nav-link" href="#talk">Talks</a></li>
                    <li class="nav-item"><a class="nav-link" href="#mentoring">Mentoring</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <header class="jumbotron" id="top">
        <div class="container">
            <div class="row row-header">
                <div class="col-12 col-sm-4 mx-auto align-self-center">
                    <img id="profile-photo" src="img/citang.jpg" class="img-fluid rounded-circle mx-auto d-block" style="max-height: 250px;" />
                </div>
                <div id="profile-info" class="col-12 col-sm mx-auto align-self-center text-lg-start text-center">

                    <h2>Chi Ian Tang</h2>
                    
                    <h5>
                        AI/ML Researcher in Mobile Systems<br/>
                        Nokia Bell Labs
                    </h5>
                    
                    <p>
                        <a class="" type="button" role="button" href="mailto:cit27@cl.cam.ac.uk"><i class="fa-solid fa-envelope fa-lg" title="Email"></i></a>
                        <a class="" type="button" role="button" href="https://scholar.google.com/citations?user=xqXm3xUAAAAJ" title="Google Scholar"><i class="fa-solid fa-graduation-cap fa-lg"></i></a>
                        <a class="" type="button" role="button" href="https://github.com/iantangc" title="GitHub"><i class="fa-brands fa-github fa-lg"></i></a>
                        <a class="" type="button" role="button" href="https://www.linkedin.com/in/iantangc" title="Linkedin"><i class="fa-brands fa-linkedin fa-lg"></i></a>
                        <a class="" type="button" role="button" href="https://x.com/iantangc" title="Twitter"><i class="fa-brands fa-x-twitter fa-lg"></i></a>
                    </p>

                    <p>
                        <a href="https://www.nokia.com/people/ian-tang/"><img src="./img/NBL_Logo_rgb.svg" class="img-fluid" style="max-width: 125px;" alt="Nokia Bell Labs"/></a>
                    </p>
                </div>
                
            </div>
        </div>
    </header>

    <div id="main_content">
        <div id="about"></div>
        <div class="container">
            
            <div class="row row-header" style="margin-top: 40px;">
                <div class="col-12">
                    <p>
                        I am a Research Scientist in AI/ML in Mobile Systems at <a href="https://www.bell-labs.com/about/locations/cambridge-uk/">Nokia Bell Labs Cambridge</a> and a former PhD student at the <a href="https://mobile-systems.cl.cam.ac.uk/people.html">Mobile Systems Research Lab</a>, University of Cambridge. 
                        
                        My work focuses on developing data-efficient and scalable machine learning algorithms, particularly for mobile systems. I specialize in developing robust AI models that can handle complex, real-world scenarios by leveraging data-efficient approaches, including semi-supervised and self-supervised learning.
                    </p>
                    <p>
                        My key research areas include:
                    </p>
                    <ul>
                        <li>
                            <b>Scalable and Data-efficient Machine Learning for Human Activity Recognition:</b> I develop novel training algorithms that reduce data dependence while ensuring robust recognition in mobile applications. My work leverages semi-supervised and self-supervised learning techniques for human activity recognition, utilizing methods such as multimodal learning [<a href="https://arxiv.org/abs/2411.15127">TSALM @ NeurIPS 2024</a>], contrastive learning [<a href="https://arxiv.org/abs/2401.02255">HCRL @ AAAI 2024</a>, <a href="https://arxiv.org/abs/2011.11542">ML4MH @ NeurIPS 2020</a>], self-training [<a href="https://doi.org/10.1145/3448112">IWMUT 2021</a>], and multi-device collaboration [<a href="https://doi.org/10.1145/3517246">IWMUT 2022</a>].
                        </li>
                        <li>
                            <b>Overcoming Catastrophic Forgetting in Continual Learning:</b> I investigate strategies to address the challenges of catastrophic forgetting in continual learning, where models must learn from evolving data streams without losing previously acquired knowledge. My work focuses on approaches that balance stability and plasticity, ensuring models can generalize effectively across new tasks while retaining performance on past tasks [<a href="https://openaccess.thecvf.com/content/WACV2024/html/Tang_Kaizen_Practical_Self-Supervised_Continual_Learning_With_Continual_Fine-Tuning_WACV_2024_paper.html">WACV 2024</a>, <a href="https://doi.org/10.1109/ICASSP43922.2022.9746862">ICASSP 2022</a>].
                        </li>
                        <li>
                            <b>Federated Learning for Scalable Learning Algorithms:</b> I explore decentralized machine learning paradigms that prioritize data privacy and enable collaborative learning across distributed devices [<a href="https://proceedings.mlr.press/v162/lubana22a.html">ICML 2022</a>].
                        </li>
                        <li>
                            <b>AI for Health-related Applications:</b> I apply machine learning techniques to health-related challenges, leveraging AI to enhance the accuracy and scalability of health monitoring applications [<a href="https://arxiv.org/abs/2111.07089">ML4H 2021</a>, <a href="https://doi.org/10.1038/s42256-020-0167-4">Nat. Mach. Intell. 2020</a>].
                        </li>
                    </ul>
                    
                    <p>
                        I am passionate about leveraging AI to advance human-centric applications, from healthcare to mobile systems. By developing robust and scalable solutions, I aim to contribute to the future of ubiquitous computing, creating technologies that seamlessly integrate into and enhance our daily lives.
                    </p>
                    <p><a href="./files/Chi_Ian_Tang_CV.pdf" class="btn btn-sm btn-outline-dark" role="button">CV <i class="fa-regular fa-file-pdf fa-lg"></i></a></p>
                </div>
            </div>
            
            <div id="news"></div>
            <div class="row row-header">
                <div class="col-12">
                    <h2>News</h2>
                    <div class="hr"></div>
                </div>
            </div>

            <div class="row">
                <div class="col-12">
                    <p>
                        <strong>May 2025</strong> - <span style="color:red;"><strong>Call for participation!</strong></span> I will be running the <a href="https://genai4hs.github.io/">GenAI4HS</a> (Generative AI and Foundation Models for Human Sensing Workshop) at UbiComp 2025 in Espoo, Finland (on 12, 13 October 2025). Preliminary and early works are welcome! <a href="https://genai4hs.github.io/">Click here to learn more!</a>
                    </p>
                    

                    <div id="more_news_content" class="collapse" aria-expanded="false">
                        <p>
                            <strong>May 2025</strong> - Papers acceptance news! Our survey paper <a href="https://arxiv.org/abs/2411.14452">Past, Present, and Future of Sensor Based Human Activity Recognition using Wearables: A Surveying Tutorial on a Still Challenging Task</a> has been accepted to IMWUT, and another paper <a href="https://doi.org/10.1145/3715014.3722079">BioQ: Towards Context-Aware Multi-Device Collaboration with Bio-cues</a> has been accepted for a presentation at SenSys.
                        </p>
                        <p>
                            <strong>December 2024</strong> - I will be attending NeurIPS in Vancouver this December with our summer intern, <a href="https://www.linkedin.com/in/arnavdas/">Arnav</a>. We will be presenting our method, <a href="https://arxiv.org/abs/2411.15127">PRetraining IMU encoderS (PRIMUS)</a>, which explores multimodal learning from text and video data for IMU time-series. The presentation will take place on <strong>Sunday, 15 December</strong>, in <strong>Meeting Room 220-222</strong> during the poster sessions: <strong>9:34 AM - 10:35 AM and 1:00 PM - 2:00 PM</strong> of the <a href="https://neurips-time-series-workshop.github.io/">TSALM workshop</a>. Feel free to drop by to discuss recent developments in multimodal foundation models for personal and wearable applications.
                        </p>
                        <p>
                            <strong>September 2024</strong> - I will be running the second version of the <a href="https://sites.google.com/view/soar-tutorial-ubicomp2024/home">UbiComp SOAR Tutorial</a> on solving the activity recognition problem from 1:00 PM to 5:00 PM on October 6 at Melbourne Australia. Come and join us for an exciting discussion!
                        </p>
                    </div>
                    
                    <p style="padding-top: 1em;">
                        <a id="more_news_btn" href="#more_news_content" role="button" data-toggle="collapse" aria-expanded="false"></a>
                    </p>
                </div>
            </div>

            <div id="publications"></div>
            <div class="row row-header">
                <div class="col-12">
                    <h2>Works/Publications</h2>
                    <div class="hr"></div>
                </div>
            </div>

            <div class="row">
                <div class="col-12">
                    <h3>2025</h3>
                    <p></p>
                </div>
            </div>

            <div class="row">
                <div class="col-12 col-md-12">
                    <p>
                        <strong>Past, Present, and Future of Sensor Based Human Activity Recognition using Wearables: A Surveying Tutorial on a Still Challenging Task</strong> <br/>
                        Harish Haresamudram, <u>Chi Ian Tang</u>, Sungho Suh, Paul Lukowicz, Thomas Ploetz<br/>
                        In <a href="https://doi.org/10.1145/3729467">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</a>. Volume 9 Issue 2, Article 34 (March 2025).
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.1145/3729467" class="btn btn-outline-dark" role="button">ACM IMWUT <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://arxiv.org/abs/2411.14452" class="btn btn-outline-dark" role="button">arXiv <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                    </div>

                </div>
            </div>
            <div class="hr row-item"></div>


            <div class="row">
                <div class="col-12 col-md-3 order-md-last">
                    <a href="https://doi.org/10.1145/3715014.3722079">
                        <img src="img/2025_BioQ.png" class="img-fluid mx-auto d-block p-1" style="max-height: 250px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>BioQ: Towards Context-Aware Multi-Device Collaboration with Bio-cues</strong> <br/>
                        Adiba Orzikulova, Diana Vasile, <u>Chi Ian Tang</u>, Fahim Kawsar, Sung-Ju Lee, Chulhong Min<br/>
                        In <a href="https://doi.org/10.1145/3715014.3722079">SenSys 2025 (ACM Conference on Embedded Networked Sensor Systems)</a><br/>
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.1145/3715014.3722079" class="btn btn-outline-dark" role="button">ACM SenSys <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://github.com/Nokia-Bell-Labs/contextual-biological-cues" class="btn btn-outline-dark" role="button">GitHub <i class="fa-brands fa-github fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12 col-md-3 order-md-last">
                    <a href="https://doi.org/10.1109/ICASSP49660.2025.10888874">
                        <img src="img/2024_PRIMUS.png" class="img-fluid mx-auto d-block p-1" style="max-height: 250px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision</strong> <br/>
                        Arnav Das, <u>Chi Ian Tang</u>, Fahim Kawsar, Mohammad Malekzadeh<br/>
                        In <a href="https://doi.org/10.1109/ICASSP49660.2025.10888874">ICASSP 2025</a> <span style="color:red;">Lecture (Oral Presentation) üèÜ</span><br/>
                        <small>Also in <a href="https://neurips-time-series-workshop.github.io/accepted-papers/">NeurIPS 2024 Workshop: Time Series in the Age of Large Models (TSALM)</a></small>
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.1109/ICASSP49660.2025.10888874" class="btn btn-outline-dark" role="button">IEEE ICASSP <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://arxiv.org/abs/2411.15127" class="btn btn-outline-dark" role="button">arXiv <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://github.com/Nokia-Bell-Labs/pretrained-imu-encoders" class="btn btn-outline-dark" role="button">GitHub <i class="fa-brands fa-github fa-lg"></i></a>
                        <a href="https://zenodo.org/records/15147513" class="btn btn-outline-dark" role="button">Model <i class="fa-solid fa-square-binary fa-lg"></i></a>
                        <a href="./files/PRIMUS_Poster.pdf" class="btn btn-outline-dark" role="button">Poster <i class="fa-regular fa-file-pdf fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12">
                    <h3>2024</h3>
                    <p></p>
                </div>
            </div>

            <div class="row">
                <div class="col-12 col-md-3 order-md-last">
                    <a href="https://sites.google.com/view/soar-tutorial-ubicomp2024/home">
                        <img src="img/UbiComp_2024.jpg" class="img-fluid mx-auto d-block p-1" style="max-height: 250px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>Solving the Sensor-Based Activity Recognition Problem (SOAR): Self-Supervised, Multi-Modal Recognition of Activities from Wearable Sensors</strong> <br/>
                        Harish Haresamudram, <u>Chi Ian Tang</u>, Sungho Suh, Paul Lukowicz, Thomas Ploetz<br/>
                        In <a href="https://doi.org/10.1145/3675094.3677562">UbiComp 2024 Companion</a>
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.1145/3675094.3677562" class="btn btn-outline-dark" role="button">ACM UbiComp Proposal <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://sites.google.com/view/soar-tutorial-ubicomp2024/home" class="btn btn-outline-dark" role="button">Website <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12">
                    <p>
                        <strong>Balancing Continual Learning and Fine-tuning for Human Activity Recognition</strong> <br/>
                        <u>Chi Ian Tang</u>, Lorena Qendro, Dimitris Spathis, Fahim Kawsar, Akhil Mathur, Cecilia Mascolo<br/>
                        In <a href="https://arxiv.org/abs/2401.02255">AAAI 2024 Workshop: Human-Centric Representation Learning (HCRL)</a>
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://arxiv.org/abs/2401.02255" class="btn btn-outline-dark" role="button">arXiv <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="./files/2024_AAAI_HCLR_Poster.pdf" class="btn btn-outline-dark" role="button">Poster <i class="fa-regular fa-file-pdf fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12 col-md-3 order-md-last">
                    <a href="https://openaccess.thecvf.com/content/WACV2024/html/Tang_Kaizen_Practical_Self-Supervised_Continual_Learning_With_Continual_Fine-Tuning_WACV_2024_paper.html">
                        <img src="img/2024_Kaizen_WACV.png" class="img-fluid mx-auto d-block p-1" style="max-height: 250px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>Kaizen: Practical Self-Supervised Continual Learning With Continual Fine-Tuning</strong> <br/>
                        <u>Chi Ian Tang</u>, Lorena Qendro, Dimitris Spathis, Fahim Kawsar, Cecilia Mascolo, Akhil Mathur<br/>
                        In <a href="https://openaccess.thecvf.com/content/WACV2024/html/Tang_Kaizen_Practical_Self-Supervised_Continual_Learning_With_Continual_Fine-Tuning_WACV_2024_paper.html">WACV 2024 (IEEE/CVF Winter Conference on Applications of Computer Vision) </a>
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.1109/WACV57701.2024.00282" class="btn btn-outline-dark" role="button">IEEE WACV <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://openaccess.thecvf.com/content/WACV2024/html/Tang_Kaizen_Practical_Self-Supervised_Continual_Learning_With_Continual_Fine-Tuning_WACV_2024_paper.html" class="btn btn-outline-dark" role="button">WACV Open Access <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://arxiv.org/abs/2303.17235" class="btn btn-outline-dark" role="button">arXiv <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://github.com/Nokia-Bell-Labs/kaizen" class="btn btn-outline-dark" role="button">GitHub <i class="fa-brands fa-github fa-lg"></i></a>
                        <a href="https://youtu.be/QGQh-9d13K8" class="btn btn-outline-dark" role="button">Presentation <i class="fa-solid fa-video fa-lg"></i></a>
                        <a href="./files/Kaizen_WACV_2024_Poster.pdf" class="btn btn-outline-dark" role="button">Poster <i class="fa-regular fa-file-pdf fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12">
                    <h3>2023</h3>
                    <p></p>
                </div>
            </div>

            <div class="row">
                <div class="col-12">
                    <p>
                        <strong>Self-supervised Learning for Data-efficient Human Activity Recognition</strong> <br/>
                        Chi Ian Tang<br/>
                        PhD Thesis
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.17863/CAM.107522" class="btn btn-outline-dark" role="button">Open Access @ University of Cambridge <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12 col-md-3 order-md-last">
                    <a href="https://sites.google.com/view/soar-tutorial-ubicomp2023/home">
                        <img src="img/UbiComp_2023.png" class="img-fluid mx-auto d-block p-1" style="max-height: 250px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>Solving the Sensor-based Activity Recognition Problem (SOAR): Self-supervised, Multi-modal Recognition of Activities from Wearable Sensors</strong> <br/>
                        Harish Haresamudram, <u>Chi Ian Tang</u>, Sungho Suh, Paul Lukowicz, Thomas Ploetz<br/>
                        In <a href="https://doi.org/10.1145/3594739.3605102">UbiComp/ISWC 2023 Adjunct</a>
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.1145/3594739.3605102" class="btn btn-outline-dark" role="button">ACM UbiComp Proposal <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://sites.google.com/view/soar-tutorial-ubicomp2023/home" class="btn btn-outline-dark" role="button">Website <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12">
                    <h3>2022</h3>
                    <p></p>
                </div>
            </div>

            <div class="row">
                <div class="col-12 col-md-3 order-md-last">
                    <a href="https://proceedings.mlr.press/v162/lubana22a.html">
                        <img src="img/ICML_orchestra.png" class="img-fluid mx-auto d-block p-1" style="max-height: 250px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering</strong> <br/>
                        Ekdeep Singh Lubana, <u>Chi Ian Tang</u>, Fahim Kawsar, Robert P. Dick, Akhil Mathur<br/>
                        In <a href="https://icml.cc/virtual/2022/spotlight/18302">ICML 2022 (International Conference on Machine Learning)</a>
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://proceedings.mlr.press/v162/lubana22a.html" class="btn btn-outline-dark" role="button">PMLR <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://arxiv.org/abs/2205.11506" class="btn btn-outline-dark" role="button">arXiv <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://github.com/akhilmathurs/orchestra" class="btn btn-outline-dark" role="button">GitHub <i class="fa-brands fa-github fa-lg"></i></a>
                        <a href="https://icml.cc/virtual/2022/spotlight/18302" class="btn btn-outline-dark" role="button">Presentation & Slides <i class="fa-solid fa-video fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12 col-md-3 order-md-last">
                    <a href="https://doi.org/10.1109/ICASSP43922.2022.9746862">
                        <img src="img/ICASSP_feature_generalizability_class_incremental_learning.png" class="img-fluid mx-auto d-block p-1" style="max-height: 250px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>Improving Feature Generalizability with Multitask Learning in Class Incremental Learning</strong> <br/>
                        Dong Ma*, <u>Chi Ian Tang</u>*, Cecilia Mascolo<br/>
                        <small><i>*Ordered alphabetically, equal contribution</i></small><br/>
                        In <a href="https://doi.org/10.1109/ICASSP43922.2022.9746862">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022</a>
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.1109/ICASSP43922.2022.9746862" class="btn btn-outline-dark" role="button">IEEE ICASSP <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://arxiv.org/abs/2204.12915" class="btn btn-outline-dark" role="button">arXiv <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://youtu.be/y8iyI96iq-U" class="btn btn-outline-dark" role="button">Presentation <i class="fa-solid fa-video fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12 col-md-3 order-md-last">
                    <a href="https://doi.org/10.1145/3517246">
                        <img src="img/ColloSSL.png" class="img-fluid mx-auto d-block p-1" style="max-height: 250px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>ColloSSL: Collaborative Self-Supervised Learning for Human Activity Recognition</strong> <br/>
                        Yash Jain*, <u>Chi Ian Tang</u>*, Chulhong Min, Fahim Kawsar, Akhil Mathur<br/>
                        <small><i>*Ordered alphabetically, equal contribution</i></small><br/>
                        In <a href="https://doi.org/10.1145/3517246">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</a>. Volume 6 Issue 1, Article 17 (March 2022).
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.1145/3517246" class="btn btn-outline-dark" role="button">ACM IMWUT <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://arxiv.org/abs/2202.00758" class="btn btn-outline-dark" role="button">arXiv <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://github.com/akhilmathurs/collossl" class="btn btn-outline-dark" role="button">https://github.com/akhilmathurs/collossl <i class="fa-brands fa-github fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12">
                    <h3>2021</h3>
                    <p></p>
                </div>
            </div>

            <div class="row">
                <div class="col-12">
                    <p>
                        <strong>Evaluating Contrastive Learning on Wearable Timeseries for Downstream Clinical Outcomes</strong> <br/>
                        Kevalee Shah, Dimitris Spathis, <u>Chi Ian Tang</u>, Cecilia Mascolo<br/>
                        In <a href="https://ml4health.github.io/2021/papers.html">Machine Learning for Health (ML4H) 2021</a>
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://arxiv.org/abs/2111.07089" class="btn btn-outline-dark" role="button">arXiv <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="./files/ML4H_2021_Poster.pdf" class="btn btn-outline-dark" role="button">Poster <i class="fa-regular fa-file-pdf fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12 col-md-3 order-md-last">
                    <a href="./files/GSL_ICML_SSLRP_2021_Tang_poster.pdf">
                        <img src="img/ICML_group_supervised.png" class="img-fluid mx-auto d-block p-1" style="max-height: 150px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>Group Supervised Learning: Extending Self-Supervised Learning to Multi-Device Settings</strong> <br/>
                        Yash Jain*, <u>Chi Ian Tang</u>*, Chulhong Min, Fahim Kawsar, Akhil Mathur<br/>
                        <small><i>*Equal Contribution</i></small><br/>
                        In <a href="https://icml21ssl.github.io/pages/Accepted%20Paper.html">ICML 2021 Workshop: Self-Supervised Learning for Reasoning and Perception</a>
                    </p>
                    <div class="btn-group-sm">
                        <a href="./files/GSL_ICML_SSLRP_2021_Tang_Paper.pdf" class="btn btn-outline-dark" role="button">Paper <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="./files/GSL_ICML_SSLRP_2021_Tang_poster.pdf" class="btn btn-outline-dark" role="button">Poster <i class="fa-regular fa-file-pdf fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12 col-md-3 order-md-last">
                    <a href="https://github.com/iantangc/SelfHAR">
                        <img src="img/SelfHAR.png" class="img-fluid mx-auto d-block p-1" style="max-height: 250px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>SelfHAR: Improving Human Activity Recognition through Self-training with Unlabeled Data</strong> <br/>
                        <u>Chi Ian Tang</u>, Ignacio Perez-Pozuelo, Dimitris Spathis, Soren Brage, Nick Wareham, Cecilia Mascolo. <br/>
                        In <a href="https://doi.org/10.1145/3448112">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</a>. Volume 5 Issue 1, Article 36 (March 2021).
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.1145/3448112" class="btn btn-outline-dark" role="button">ACM IMWUT <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://github.com/iantangc/SelfHAR" class="btn btn-outline-dark" role="button">GitHub <i class="fa-brands fa-github fa-lg"></i></a>
                        <a href="https://www.youtube.com/watch?v=WpZCiUXUNAo" class="btn btn-outline-dark" role="button">Presentation (Short) <i class="fa-solid fa-video fa-lg"></i></a>
                        <a href="https://www.youtube.com/watch?v=BzBLSH13kzU&list=PLLYQxSghuHCJe5gEw7Zp-ylJfhaAJUORi" class="btn btn-outline-dark" role="button">Presentation (Full) <i class="fa-solid fa-video fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12">
                    <h3>2020</h3>
                    <p></p>
                </div>
            </div>

            <div class="row">

                <div class="col-12 col-md-3 order-md-last">
                    <a href="https://github.com/iantangc/ContrastiveLearningHAR">
                        <img src="img/SimCLR_HAR.png" class="img-fluid mx-auto d-block p-1" style="max-height: 250px;" />
                    </a>
                </div>

                <div class="col-12 col-md-9">
                    <p>
                        <strong>Exploring Contrastive Learning in Human Activity Recognition for Healthcare</strong> <br/>
                        <u>Chi Ian Tang</u>, Dimitris Spathis, Ignacio Perez Pozuelo, Cecilia Mascolo. <br/>
                        In <a href="https://sites.google.com/view/ml4mobilehealth-neurips-2020/home">ML for Mobile Health Workshop at NeurIPS</a>. 2020.
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://arxiv.org/abs/2011.11542" class="btn btn-outline-dark" role="button">arXiv <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://github.com/iantangc/ContrastiveLearningHAR" class="btn btn-outline-dark" role="button">GitHub <i class="fa-brands fa-github fa-lg"></i></a>
                        <a href="./files/ML4MH_NeurIPS_2020_Tang_Poster.pdf" class="btn btn-outline-dark" role="button">Poster <i class="fa-regular fa-file-pdf fa-lg"></i></a>
                    </div>
                </div>
            </div>
            <div class="hr row-item"></div>

            <div class="row">
                <div class="col-12">
                    <p>
                        <strong>Exploring the limit of using a deep neural network on pileup data for germline variant calling</strong> <br/>
                        Ruibang Luo, Chak-Lim Wong, Yat-Sing Wong, <u>Chi-Ian Tang</u>, Chi-Man Liu, Henry CM Leung, Tak-Wah Lam. <br/>
                        In Nature Machine Intelligence. 2020.
                    </p>
                    <div class="btn-group-sm">
                        <a href="https://doi.org/10.1038/s42256-020-0167-4" class="btn btn-outline-dark" role="button">Nat. Mach. Intell. <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a>
                        <a href="https://github.com/HKU-BAL/Clair" class="btn btn-outline-dark" role="button">GitHub <i class="fa-brands fa-github fa-lg"></i></a>
                    </div>
                </div>
            </div>

            <div id="service"></div>
            <div class="row row-header">
                <div class="col-12">
                    <h2>Academic Service</h2>
                    <div class="hr"></div>
                </div>
            </div>
        
            <div class="row">
                <div class="col-12">
                    <p>
                        I have taken up roles for the following:
                        <ul>
                            <li>
                                Associate Editor of <a href="https://dl.acm.org/journal/imwut/editorial-board">ACM IMWUT</a> (2025-)
                            </li>
                            <li>
                                Program Committee of <a href="https://www.sigmobile.org/mobisys/2025/program_committee/">MobiSys</a> (2025-)
                            </li>
                            <li>
                                Technical Program Committee of <a href="https://autocare.ai/abc2025">ABC</a> (2025-) <span style="color: rgb(229, 166, 0);">(Elite Reviewer) üèÜ</span><br/>
                            </li>
                            <li>
                                Co-chair of the <a href="https://genai4hs.github.io/">Generative AI and Foundation Models for Human Sensing Workshop (GenAI4HS)</a> at UbiComp 2025
                            </li>
                            <li>
                                Organizer of UbiComp Tutorial on Solving the sensor-based activity recognition problem (SOAR) (<a href="https://sites.google.com/view/soar-tutorial-ubicomp2023/home">2023</a> Cancun, Mexico, <a href="https://sites.google.com/view/soar-tutorial-ubicomp2024/home">2024</a> Melbourne, Australia)
                            </li>
                            <li>
                                Organizer of <a href="https://hcrl-workshop.github.io/2024/">HCRL workshop at AAAI 2024, Vancouver, Canada</a>
                            </li>
                        </ul>
                    </p>
                </div>
            </div>

            <div id="talk"></div>
            <div class="row row-header">
                <div class="col-12">
                    <h2>Invited Talks</h2>
                    <div class="hr"></div>
                </div>
            </div>

            <div class="row">
                <div class="col-12">
                    <ul>
                        <li>
                            <a href="http://xai.korea.ac.kr/kor/borad/notice?viewMode=view&ca=&sel_search=&txt_search=&page=1&idx=128">The Future of Wearable Health: Mobile AI Research at Nokia Bell Labs</a> 
                            <div style="float: right"><a href="./img/2025_05_28_Korea_University_Invited_Talk.png" class="btn btn-outline-dark btn-inline-link" role="button">Poster <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a> </div> <br>
                            <em>At Korea University, South Korea, <small> 2025 May 28</small></em>     
                        </li>
                        <li>
                            <a href="https://www.fst.um.edu.mo/event/data-efficient-ai-and-multimodal-learning-unlocking-the-potential-of-mobile-sensing-for-health-insights/">Data-Efficient AI and Multimodal Learning: Unlocking the Potential of Mobile Sensing for Health Insights</a>
                            <div style="float: right"><a href="./img/2025_01_10_University_of_Macau_Invited_Talk.jpg" class="btn btn-outline-dark btn-inline-link" role="button">Poster <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a></div><br>
                            <em>At University of Macau, Macao SAR, <small> 2025 January 10</small></em>
                        </li>
                    </ul>
                    <p></p>
                    
                </div>
            </div>

            <div id="mentoring"></div>
            <div class="row row-header">
                <div class="col-12">
                    <h2>Mentoring and Teaching</h2>
                    <div class="hr"></div>
                </div>
            </div>
        
            <div class="row">
                <div class="col-12">
                    <h5>Mentoring</h5>
                    <p>I truly enjoy and always learn a lot in mentoring PhD students. Here are some research projects I have had the pleasure of supervising:</p>
                    <p>
                        <ul>
                            <li>
                                <a href="https://scholar.google.com/citations?user=rnRml4EAAAAJ">Arnav Das</a> (University of Washington): Multimodal learning for mobile sensing
                            </li>
                        </ul>
                    </p>

                    <h5>Lectures/Tutorials</h5>
                    <p>
                        I gave a lecture/tutorial on <a href="https://www.cl.cam.ac.uk/teaching/2223/MH/MH-practical2.pdf">Machine Learning and Features of Health Data</a> in the <a href="https://www.cl.cam.ac.uk/teaching/2223/L349/">Mobile Health course</a> (Master's level) at the Department of Computer Science and Technology, University of Cambridge. Slides are available <a href="./files/2023-MH-practical2.pdf" class="btn btn-outline-dark  btn-inline-link" role="button">Slides <i class="fa-solid fa-arrow-up-right-from-square fa-lg"></i></a> (2023)
                    </p>
                    <h5 style="margin-top: 2rem;">Supervisions</h5>
                    <p>
                        I have supervised students and demonstrated for the following courses at the University of Cambridge:
                        <ul>
                            <li>
                                <a href="https://www.cl.cam.ac.uk/teaching/2021/ProgC/">Programming in C and C++</a> (2020-2021)
                            </li>
                            <li>
                                <a href="https://www.cl.cam.ac.uk/teaching/2021/MLRD/">Machine Learning and Real-world Data</a> (2019-2021)
                            </li>
                            <li>
                                <a href="https://www.cl.cam.ac.uk/teaching/2122/CompTheory/">Computation Theory</a> (2021-2024)
                            </li>
                            <li>
                                <a href="https://www.cl.cam.ac.uk/teaching/2223/DigElec/">Digital Electronics</a> (2022-2023)
                            </li>
                        </ul>
                    </p>
                    <h5 style="margin-top: 2rem;">Teaching Materials</h5>
                    <ul>
                        <li>
                            <a href="./files/How_To_Use_The_Y_Combinator_Chi_Ian_Tang_20220330.pdf">How to Use the Y Combinator</a> (for Computation Theory 2021-2022)
                        </li>
                    </ul>
                </div>
            </div>


            <div class="row d-flex justify-content-center p-2">
                <div class="col-auto d-flex justify-content-center">
                    <p class="text-muted"> ¬© 2020-2025 Chi Ian Tang. All rights reserved.
                    </p>
                </div>
            </div>

        </div>
    </div>



    <footer class="footer">
        <div class="container">
            <div class="row">
                <div class="col-auto d-flex justify-content-center">
                <a class="btn btn-social-icon" type="button" role="button" href="mailto:cit27@cl.cam.ac.uk"><i class="fa-solid fa-envelope fa-lg" title="Email"></i></a>
                <a class="btn btn-social-icon" type="button" role="button" href="https://scholar.google.com/citations?user=xqXm3xUAAAAJ" title="Google Scholar"><i class="fa-solid fa-graduation-cap fa-lg"></i></a>
                <a class="btn btn-social-icon" type="button" role="button" href="https://github.com/iantangc" title="GitHub"><i class="fa-brands fa-github fa-lg"></i></a>
                <a class="btn btn-social-icon" type="button" role="button" href="https://www.linkedin.com/in/iantangc" title="Linkedin"><i class="fa-brands fa-linkedin fa-lg"></i></a>
                <a class="btn btn-social-icon" type="button" role="button" href="https://x.com/iantangc" title="Twitter"><i class="fa-brands fa-x-twitter fa-lg"></i></a>
                </div>
            </div>

        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script src="js/scripts.js"></script>
</body>

</html>
